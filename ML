import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.feature_selection import VarianceThreshold
import hdbscan
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# === Leitura do CSV ===
df = pd.read_csv('INFO-Sumario_Taxa_Setor-2025-07-15.csv', encoding='ISO-8859-1', sep=';')

# === Colunas de interesse ===
indices_cols = [
    'Indice_Taxa', 'Indice_Indisp', 'TX_Indice', 'Indice_TX_PktLoss',
    'Indice_TX_Delay', 'PRB_Indice', 'UE_Indice', 'Indice_RRC',
    'Indice_ERAB', 'Indice_CSFB', 'Indice_SIGS1', 'Indice_QUEDA.SESSAO',
    'Indice_ERAB.QCI1', 'Indice_DROP.RATE.QCI1', 'Indice_INTERFERÃŠNCIA',
    'Indice_HO.VOLTE', 'Indice_HANDOVER', 'Indice_CQI', 'Indice_PACKET.DELAY',
    'Indice_UTIL.PDCCH', 'Indice_LATENCIA.BUFFER.DL',
    'Indice_REASSEMBLY.FAIL.RATE', 'Indice_FRAGMENTACAO.TX',
    'Indice_DESVIO.CLOCK'
]

# === ConversÃ£o dos dados ===
for col in indices_cols:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.').str.strip(), errors='coerce')

print(f"=== DIAGNÃ“STICO DOS DADOS ===")
print(f"Shape original: {df.shape}")

# === AnÃ¡lise de missings ===
missing_analysis = df[indices_cols].isnull().sum()
print(f"\nValores faltantes por coluna:")
for col, missing in missing_analysis.items():
    if missing > 0:
        pct = missing / len(df) * 100
        print(f"  {col}: {missing} ({pct:.1f}%)")

# === Remover colunas com muitos missings ===
cols_to_keep = []
for col in indices_cols:
    missing_pct = df[col].isnull().sum() / len(df)
    if missing_pct < 0.5:  # Manter colunas com menos de 50% missing
        cols_to_keep.append(col)
    else:
        print(f"Removendo {col} - muitos valores faltantes ({missing_pct*100:.1f}%)")

indices_cols = cols_to_keep
print(f"Colunas mantidas: {len(indices_cols)}")

# === AnÃ¡lise de variÃ¢ncia ===
# Remover colunas com baixa variÃ¢ncia (todos valores iguais ou quase)
df_temp = df[indices_cols].fillna(df[indices_cols].median())
variance_selector = VarianceThreshold(threshold=0.01)  # VariÃ¢ncia mÃ­nima
variance_selector.fit(df_temp)

low_variance_cols = [col for i, col in enumerate(indices_cols) 
                    if not variance_selector.get_support()[i]]
if low_variance_cols:
    print(f"Removendo colunas com baixa variÃ¢ncia: {low_variance_cols}")
    indices_cols = [col for col in indices_cols if col not in low_variance_cols]

# === Remover outliers extremos ANTES do clustering ===
print(f"\n=== TRATAMENTO DE OUTLIERS ===")
df_clean = df.copy()

# Para cada Ã­ndice, remover valores impossÃ­veis (fora de 0-10)
for col in indices_cols:
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= 0) & (df_clean[col] <= 10))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} valores fora do range 0-10")

# Remover outliers usando IQR para cada coluna
for col in indices_cols:
    Q1 = df_clean[col].quantile(0.05)
    Q3 = df_clean[col].quantile(0.95)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} outliers extremos")

print(f"Dados apÃ³s limpeza: {len(df_clean)} linhas ({len(df_clean)/len(df)*100:.1f}% do original)")

# === RemoÃ§Ã£o de correlaÃ§Ã£o alta ===
corr_matrix = df_clean[indices_cols].corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]  # Menor threshold
filtered_cols = [col for col in indices_cols if col not in to_drop]

if to_drop:
    print(f"\nColunas removidas por alta correlaÃ§Ã£o (>0.8): {to_drop}")
print(f"Colunas finais para clustering: {len(filtered_cols)}")

# === PreparaÃ§Ã£o final dos dados ===
X_final = df_clean[filtered_cols].fillna(df_clean[filtered_cols].median())
print(f"Dados finais para clustering: {X_final.shape}")

# === Teste com mÃºltiplos algoritmos ===
print(f"\n=== TESTANDO DIFERENTES ALGORITMOS ===")

# Preparar pipeline de prÃ©-processamento
preprocessor = Pipeline([
    ("scaler", StandardScaler())
])

X_scaled = preprocessor.fit_transform(X_final)

# 1. KMeans com diferentes nÃºmeros de clusters
print("\n1. TESTANDO K-MEANS:")
silhouette_scores = []
k_range = range(3, 15)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    ch_score = calinski_harabasz_score(X_scaled, labels)
    silhouette_scores.append(score)
    print(f"  K={k}: Silhouette={score:.3f}, CH_Score={ch_score:.1f}")

# Melhor K para KMeans
best_k = k_range[np.argmax(silhouette_scores)]
print(f"  Melhor K para K-Means: {best_k} (Score: {max(silhouette_scores):.3f})")

# 2. DBSCAN
print("\n2. TESTANDO DBSCAN:")
eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]
min_samples_values = [5, 10, 20]

best_dbscan_score = -1
best_dbscan_params = None

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        noise_pct = n_noise / len(labels) * 100
        
        if n_clusters > 1 and noise_pct < 30:  # Menos de 30% ruÃ­do
            try:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  eps={eps}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, RuÃ­do={noise_pct:.1f}%")
                
                if score > best_dbscan_score:
                    best_dbscan_score = score
                    best_dbscan_params = (eps, min_samples)
            except:
                continue

# 3. HDBSCAN mais conservador
print("\n3. TESTANDO HDBSCAN (CONSERVADOR):")
min_cluster_sizes = [50, 100, 200]  # Clusters maiores
min_samples_values = [5, 10, 20]

best_hdbscan_score = -1
best_hdbscan_params = None

for min_size in min_cluster_sizes:
    for min_samples in min_samples_values:
        try:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_size,
                min_samples=min_samples,
                cluster_selection_epsilon=0.0
            )
            labels = clusterer.fit_predict(X_scaled)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            noise_pct = n_noise / len(labels) * 100
            
            if n_clusters > 1 and noise_pct < 30:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  min_size={min_size}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, RuÃ­do={noise_pct:.1f}%")
                
                if score > best_hdbscan_score:
                    best_hdbscan_score = score
                    best_hdbscan_params = (min_size, min_samples)
        except:
            continue

# === Escolher melhor algoritmo ===
algorithms_scores = {
    'KMeans': max(silhouette_scores),
    'DBSCAN': best_dbscan_score if best_dbscan_params else -1,
    'HDBSCAN': best_hdbscan_score if best_hdbscan_params else -1
}

print(f"\n=== COMPARAÃ‡ÃƒO FINAL ===")
for algo, score in algorithms_scores.items():
    print(f"{algo}: {score:.3f}")

best_algorithm = max(algorithms_scores, key=algorithms_scores.get)
print(f"Melhor algoritmo: {best_algorithm}")

# === Aplicar melhor algoritmo ===
if best_algorithm == 'KMeans':
    final_clusterer = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando K-Means com K={best_k}")

elif best_algorithm == 'DBSCAN':
    eps, min_samples = best_dbscan_params
    final_clusterer = DBSCAN(eps=eps, min_samples=min_samples)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando DBSCAN com eps={eps}, min_samples={min_samples}")

else:  # HDBSCAN
    min_size, min_samples = best_hdbscan_params
    final_clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_size,
        min_samples=min_samples,
        cluster_selection_epsilon=0.0
    )
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando HDBSCAN com min_size={min_size}, min_samples={min_samples}")

# === Adicionar resultados ao DataFrame ===
df_result = df_clean.copy()
df_result['cluster'] = final_labels

# === PCA para visualizaÃ§Ã£o ===
pca_viz = PCA(n_components=2)
X_pca_viz = pca_viz.fit_transform(X_scaled)
df_result['pca_1'] = X_pca_viz[:, 0]
df_result['pca_2'] = X_pca_viz[:, 1]

# === AnÃ¡lise final ===
print(f"\n=== RESULTADO FINAL ===")
n_clusters = len(set(final_labels)) - (1 if -1 in final_labels else 0)
n_noise = list(final_labels).count(-1) if -1 in final_labels else 0
noise_pct = n_noise / len(final_labels) * 100

print(f"Algoritmo usado: {best_algorithm}")
print(f"NÃºmero de clusters: {n_clusters}")
print(f"Pontos de ruÃ­do: {n_noise} ({noise_pct:.1f}%)")
print(f"Silhouette Score: {algorithms_scores[best_algorithm]:.3f}")

# DistribuiÃ§Ã£o dos clusters
cluster_counts = Counter(final_labels)
print(f"\nDistribuiÃ§Ã£o dos clusters:")
for cluster in sorted(cluster_counts.keys()):
    count = cluster_counts[cluster]
    pct = count / len(final_labels) * 100
    if cluster == -1:
        print(f"  RuÃ­do: {count} setores ({pct:.1f}%)")
    else:
        print(f"  Cluster {cluster}: {count} setores ({pct:.1f}%)")

# === AnÃ¡lise dos clusters ===
if n_clusters > 0 and n_clusters < 20:  # SÃ³ analizar se temos um nÃºmero razoÃ¡vel de clusters
    print(f"\n=== ANÃLISE DOS CLUSTERS ===")
    
    for cluster_id in sorted(set(final_labels)):
        if cluster_id == -1:
            continue
            
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        print(f"\n--- CLUSTER {cluster_id} ({len(cluster_data)} setores) ---")
        
        # MÃ©dias dos Ã­ndices
        medias = cluster_data[filtered_cols].mean()
        piores = medias.nlargest(3)
        melhores = medias.nsmallest(3)
        
        print("Ãndices mais problemÃ¡ticos:")
        for idx, valor in piores.items():
            print(f"  {idx}: {valor:.2f}")
        
        print("Ãndices com melhor performance:")
        for idx, valor in melhores.items():
            print(f"  {idx}: {valor:.2f}")

# === VisualizaÃ§Ã£o ===
plt.figure(figsize=(12, 8))

if n_clusters <= 20:  # SÃ³ plotar se nÃ£o temos muitos clusters
    unique_clusters = sorted(set(final_labels))
    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_clusters)))
    
    for i, cluster_id in enumerate(unique_clusters):
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        if cluster_id == -1:
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'], 
                       c='red', marker='x', s=30, alpha=0.6, label=f'RuÃ­do ({len(cluster_data)})')
        else:
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'],
                       c=[colors[i]], s=50, alpha=0.7, 
                       label=f'Cluster {cluster_id} ({len(cluster_data)})')
    
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering Final - {best_algorithm}')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
else:
    # Se muitos clusters, fazer visualizaÃ§Ã£o simplificada
    plt.scatter(df_result['pca_1'], df_result['pca_2'], 
               c=final_labels, cmap='tab20', s=50, alpha=0.7)
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering Final - {best_algorithm} ({n_clusters} clusters)')
    plt.colorbar()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# === Salvar resultados COMPLETOS ===
# Primeiro, vamos mapear os clusters de volta para o DataFrame original
print(f"\n=== PREPARANDO RESULTADO FINAL COMPLETO ===")

# Identificar coluna de setor
if 'SETOR' in df.columns:
    setor_col = 'SETOR'
elif 'Setor' in df.columns:
    setor_col = 'Setor'
else:
    setor_cols = [col for col in df.columns if 'setor' in col.lower()]
    if setor_cols:
        setor_col = setor_cols[0]
        print(f"Usando coluna de setor: {setor_col}")
    else:
        # Se nÃ£o encontrar coluna de setor, usar Ã­ndice original
        df['SETOR_ID'] = df.index
        setor_col = 'SETOR_ID'
        print("Criando coluna SETOR_ID baseada no Ã­ndice")

# Criar DataFrame de mapeamento cluster
cluster_mapping = df_result[[setor_col, 'cluster']].copy()

# Fazer merge com DataFrame original para manter TODAS as colunas
df_original_completo = df.copy()

# Merge para adicionar informaÃ§Ãµes de cluster ao DataFrame original
resultado_completo = df_original_completo.merge(
    cluster_mapping, 
    on=setor_col, 
    how='left'  # Left join para manter todos os setores originais
)

# Preencher NaN nos clusters (setores que foram removidos durante limpeza)
resultado_completo['cluster'] = resultado_completo['cluster'].fillna(-999)  # -999 = removido durante limpeza

# Reorganizar colunas: setor, cluster, depois todas as outras
colunas_ordenadas = [setor_col, 'cluster'] + [col for col in resultado_completo.columns 
                                              if col not in [setor_col, 'cluster']]

resultado_final_completo = resultado_completo[colunas_ordenadas].copy()

# Adicionar metadados do cluster
cluster_info = {
    0: "CRÃTICO - Prioridade ALTA",
    1: "MODERADO - Prioridade MÃ‰DIA", 
    2: "UE/DELAY - Prioridade MÃ‰DIA",
    3: "BOM - Prioridade BAIXA",
    -999: "REMOVIDO_LIMPEZA - Outlier extremo"
}

resultado_final_completo['cluster_nome'] = resultado_final_completo['cluster'].map(cluster_info)
resultado_final_completo['cluster_nome'] = resultado_final_completo['cluster_nome'].fillna('DESCONHECIDO')

# Salvar resultado completo
resultado_final_completo.to_csv('clustering_resultado_completo.csv', index=False, sep=';', encoding='utf-8')

print(f"âœ… Arquivo completo salvo: 'clustering_resultado_completo.csv'")
print(f"ðŸ“Š Colunas no arquivo: {len(resultado_final_completo.columns)}")
print(f"ðŸ“‹ Linhas no arquivo: {len(resultado_final_completo)}")
print(f"ðŸ—ï¸ Estrutura: [{setor_col}] + [cluster] + [cluster_nome] + [todas as {len(df.columns)} colunas originais]")

# EstatÃ­sticas do resultado final
print(f"\n=== ESTATÃSTICAS DO RESULTADO COMPLETO ===")
cluster_stats = resultado_final_completo['cluster'].value_counts().sort_index()
for cluster_id, count in cluster_stats.items():
    pct = count / len(resultado_final_completo) * 100
    nome = cluster_info.get(cluster_id, 'DESCONHECIDO')
    print(f"Cluster {cluster_id}: {count:,} setores ({pct:.1f}%) - {nome}")

# Mostrar primeiras colunas como exemplo
print(f"\n=== PREVIEW DAS PRIMEIRAS COLUNAS ===")
preview_cols = resultado_final_completo.columns[:10].tolist()
if len(resultado_final_completo.columns) > 10:
    preview_cols.append(f"... e mais {len(resultado_final_completo.columns) - 10} colunas")
print(f"Colunas: {preview_cols}")

print(f"\nArquivos salvos:")
print(f"ðŸ“„ 'clustering_resultado_completo.csv' - TODAS as colunas originais + cluster")
print(f"ðŸ“Š Algoritmo: {best_algorithm}")
print(f"ðŸŽ¯ Score: {algorithms_scores[best_algorithm]:.3f}")
print(f"ðŸ”¢ Clusters: {n_clusters}")
print(f"ðŸš« RuÃ­do: {noise_pct:.1f}%")
print(f"ðŸ“‹ Total de setores: {len(resultado_final_completo):,}")
print(f"ðŸ“Š Total de colunas: {len(resultado_final_completo.columns)}")
