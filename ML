import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.feature_selection import VarianceThreshold
import hdbscan
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# === Leitura do CSV ===
df = pd.read_csv('INFO-Sumario_Taxa_Setor-2025-08-06.csv', encoding='ISO-8859-1', sep=';')

# === CRIAÇÃO DA CHAVE ÚNICA ===
print(f"=== CRIANDO CHAVE ÚNICA ===")

# Verificar se as colunas necessárias existem
required_cols = ['Freq', 'Chave_Site', 'Setor']
missing_cols = [col for col in required_cols if col not in df.columns]

if missing_cols:
    print(f"❌ ERRO: Colunas obrigatórias não encontradas: {missing_cols}")
    print(f"Colunas disponíveis: {list(df.columns)}")
    exit()

# Criar chave única
df['ID_UNICO'] = df['Freq'].astype(str) + '_' + df['Chave_Site'].astype(str) + '_' + df['Setor'].astype(str)


print(f"✅ Chave única criada: ID_UNICO")
print(f"Exemplo de IDs únicos:")
for i, id_unico in enumerate(df['ID_UNICO'].head(5)):
    print(f"  {i+1}. {id_unico}")

# Verificar duplicatas
duplicatas = df['ID_UNICO'].duplicated().sum()
if duplicatas > 0:
    print(f"⚠️  AVISO: Encontradas {duplicatas} duplicatas na chave única!")
    print("Removendo duplicatas (mantendo primeira ocorrência)...")
    df = df.drop_duplicates(subset='ID_UNICO', keep='first')
    print(f"Dados após remoção de duplicatas: {len(df)} registros")
else:
    print(f"✅ Nenhuma duplicata encontrada - {len(df)} registros únicos")

# === Colunas de interesse ===
indices_cols = [
    'Indice_Taxa', 'Indice_Indisp', 'TX_Indice', 'Indice_TX_PktLoss',
    'Indice_TX_Delay', 'PRB_Indice', 'UE_Indice', 'Indice_RRC',
    'Indice_ERAB', 'Indice_CSFB', 'Indice_SIGS1', 'Indice_QUEDA.SESSAO',
    'Indice_ERAB.QCI1', 'Indice_DROP.RATE.QCI1', 'Indice_INTERFERÊNCIA',
    'Indice_HO.VOLTE', 'Indice_HANDOVER', 'Indice_CQI', 'Indice_PACKET.DELAY',
    'Indice_UTIL.PDCCH', 'Indice_LATENCIA.BUFFER.DL',
    'Indice_REASSEMBLY.FAIL.RATE', 'Indice_FRAGMENTACAO.TX',
    'Indice_DESVIO.CLOCK'
]

# === Conversão dos dados ===
for col in indices_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.').str.strip(), errors='coerce')

# Filtrar apenas colunas que existem no DataFrame
indices_cols = [col for col in indices_cols if col in df.columns]

print(f"\n=== DIAGNÓSTICO DOS DADOS ===")
print(f"Shape original: {df.shape}")
print(f"Colunas de índices encontradas: {len(indices_cols)}")
print(f"IDs únicos: {df['ID_UNICO'].nunique()}")

# === Análise de missings ===
missing_analysis = df[indices_cols].isnull().sum()
print(f"\nValores faltantes por coluna:")
for col, missing in missing_analysis.items():
    if missing > 0:
        pct = missing / len(df) * 100
        print(f"  {col}: {missing} ({pct:.1f}%)")

# === Remover colunas com muitos missings ===
cols_to_keep = []
for col in indices_cols:
    missing_pct = df[col].isnull().sum() / len(df)
    if missing_pct < 0.5:  # Manter colunas com menos de 50% missing
        cols_to_keep.append(col)
    else:
        print(f"Removendo {col} - muitos valores faltantes ({missing_pct*100:.1f}%)")

indices_cols = cols_to_keep
print(f"Colunas mantidas: {len(indices_cols)}")

if len(indices_cols) == 0:
    print("❌ ERRO: Nenhuma coluna de índices válida encontrada!")
    exit()

# === Análise de variância ===
df_temp = df[indices_cols].fillna(df[indices_cols].median())
variance_selector = VarianceThreshold(threshold=0.01)
variance_selector.fit(df_temp)

low_variance_cols = [col for i, col in enumerate(indices_cols) 
                    if not variance_selector.get_support()[i]]
if low_variance_cols:
    print(f"Removendo colunas com baixa variância: {low_variance_cols}")
    indices_cols = [col for col in indices_cols if col not in low_variance_cols]

# === Remover outliers extremos ANTES do clustering ===
print(f"\n=== TRATAMENTO DE OUTLIERS ===")
df_clean = df.copy()

# Para cada índice, remover valores impossíveis (fora de 0-10)
for col in indices_cols:
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= 0) & (df_clean[col] <= 10))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} valores fora do range 0-10")

# Remover outliers usando IQR para cada coluna
for col in indices_cols:
    Q1 = df_clean[col].quantile(0.05)
    Q3 = df_clean[col].quantile(0.95)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} outliers extremos")

print(f"Dados após limpeza: {len(df_clean)} IDs únicos ({len(df_clean)/len(df)*100:.1f}% do original)")

# === Remoção de correlação alta ===
corr_matrix = df_clean[indices_cols].corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]
filtered_cols = [col for col in indices_cols if col not in to_drop]

if to_drop:
    print(f"\nColunas removidas por alta correlação (>0.8): {to_drop}")
print(f"Colunas finais para clustering: {len(filtered_cols)}")

# === Preparação final dos dados ===
X_final = df_clean[filtered_cols].fillna(df_clean[filtered_cols].median())
print(f"Dados finais para clustering: {X_final.shape}")

# === Teste com múltiplos algoritmos ===
print(f"\n=== TESTANDO DIFERENTES ALGORITMOS ===")

# Preparar pipeline de pré-processamento
preprocessor = Pipeline([
    ("scaler", StandardScaler())
])

X_scaled = preprocessor.fit_transform(X_final)

# 1. KMeans com diferentes números de clusters
print("\n1. TESTANDO K-MEANS:")
silhouette_scores = []
k_range = range(3, 15)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    ch_score = calinski_harabasz_score(X_scaled, labels)
    silhouette_scores.append(score)
    print(f"  K={k}: Silhouette={score:.3f}, CH_Score={ch_score:.1f}")

# Melhor K para KMeans
best_k = k_range[np.argmax(silhouette_scores)]
print(f"  Melhor K para K-Means: {best_k} (Score: {max(silhouette_scores):.3f})")

# 2. DBSCAN
print("\n2. TESTANDO DBSCAN:")
eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]
min_samples_values = [5, 10, 20]

best_dbscan_score = -1
best_dbscan_params = None

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        noise_pct = n_noise / len(labels) * 100
        
        if n_clusters > 1 and noise_pct < 30:
            try:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  eps={eps}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, Ruído={noise_pct:.1f}%")
                
                if score > best_dbscan_score:
                    best_dbscan_score = score
                    best_dbscan_params = (eps, min_samples)
            except:
                continue

# 3. HDBSCAN mais conservador
print("\n3. TESTANDO HDBSCAN (CONSERVADOR):")
min_cluster_sizes = [50, 100, 200]
min_samples_values = [5, 10, 20]

best_hdbscan_score = -1
best_hdbscan_params = None

for min_size in min_cluster_sizes:
    for min_samples in min_samples_values:
        try:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_size,
                min_samples=min_samples,
                cluster_selection_epsilon=0.0
            )
            labels = clusterer.fit_predict(X_scaled)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            noise_pct = n_noise / len(labels) * 100
            
            if n_clusters > 1 and noise_pct < 30:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  min_size={min_size}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, Ruído={noise_pct:.1f}%")
                
                if score > best_hdbscan_score:
                    best_hdbscan_score = score
                    best_hdbscan_params = (min_size, min_samples)
        except:
            continue

# === Escolher melhor algoritmo ===
algorithms_scores = {
    'KMeans': max(silhouette_scores),
    'DBSCAN': best_dbscan_score if best_dbscan_params else -1,
    'HDBSCAN': best_hdbscan_score if best_hdbscan_params else -1
}

print(f"\n=== COMPARAÇÃO FINAL ===")
for algo, score in algorithms_scores.items():
    print(f"{algo}: {score:.3f}")

best_algorithm = max(algorithms_scores, key=algorithms_scores.get)
print(f"Melhor algoritmo: {best_algorithm}")

# === Aplicar melhor algoritmo ===
if best_algorithm == 'KMeans':
    final_clusterer = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando K-Means com K={best_k}")

elif best_algorithm == 'DBSCAN':
    eps, min_samples = best_dbscan_params
    final_clusterer = DBSCAN(eps=eps, min_samples=min_samples)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando DBSCAN com eps={eps}, min_samples={min_samples}")

else:  # HDBSCAN
    min_size, min_samples = best_hdbscan_params
    final_clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_size,
        min_samples=min_samples,
        cluster_selection_epsilon=0.0
    )
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando HDBSCAN com min_size={min_size}, min_samples={min_samples}")

# === Adicionar resultados ao DataFrame ===
df_result = df_clean.copy()
df_result['cluster'] = final_labels

# === PCA para visualização ===
pca_viz = PCA(n_components=2)
X_pca_viz = pca_viz.fit_transform(X_scaled)
df_result['pca_1'] = X_pca_viz[:, 0]
df_result['pca_2'] = X_pca_viz[:, 1]

# === ANÁLISE DETALHADA DO PCA ===
print(f"\n=== ANÁLISE DETALHADA DO PCA ===")

# 1. Variância explicada por cada componente
variancia_explicada = pca_viz.explained_variance_ratio_
print(f"PCA Component 1 (Eixo X): explica {variancia_explicada[0]*100:.1f}% da variação")
print(f"PCA Component 2 (Eixo Y): explica {variancia_explicada[1]*100:.1f}% da variação")
print(f"Total explicado: {sum(variancia_explicada)*100:.1f}% da variação original")

# 2. Quais índices mais influenciam cada componente
componentes = pca_viz.components_  # Shape: (2, n_features)

print(f"\n--- PCA COMPONENT 1 (Eixo X) ---")
print("Índices que MAIS influenciam este eixo:")
# Pegar os 5 índices com maior peso (positivo ou negativo)
component1_weights = componentes[0]
indices_importantes_c1 = []
for i, peso in enumerate(component1_weights):
    indices_importantes_c1.append((filtered_cols[i], peso))

# Ordenar por valor absoluto (importância)
indices_importantes_c1.sort(key=lambda x: abs(x[1]), reverse=True)

for indice, peso in indices_importantes_c1[:5]:
    sinal = "↑" if peso > 0 else "↓"
    print(f"  {indice}: {peso:.3f} {sinal}")

print(f"\n--- PCA COMPONENT 2 (Eixo Y) ---")
print("Índices que MAIS influenciam este eixo:")
component2_weights = componentes[1]
indices_importantes_c2 = []
for i, peso in enumerate(component2_weights):
    indices_importantes_c2.append((filtered_cols[i], peso))

indices_importantes_c2.sort(key=lambda x: abs(x[1]), reverse=True)

for indice, peso in indices_importantes_c2[:5]:
    sinal = "↑" if peso > 0 else "↓"
    print(f"  {indice}: {peso:.3f} {sinal}")

# 3. Interpretação prática
print(f"\n=== INTERPRETAÇÃO PRÁTICA ===")
print("🔍 COMO LER O GRÁFICO:")
print(f"📈 Eixo X (PCA 1): Representa principalmente '{indices_importantes_c1[0][0].replace('Indice_', '')}'")
print(f"📊 Eixo Y (PCA 2): Representa principalmente '{indices_importantes_c2[0][0].replace('Indice_', '')}'")

print(f"\n🎯 SIGNIFICADO DAS POSIÇÕES:")
print(f"• Pontos à DIREITA: Problemas em {indices_importantes_c1[0][0].replace('Indice_', '')}")
print(f"• Pontos à ESQUERDA: Melhores em {indices_importantes_c1[0][0].replace('Indice_', '')}")
print(f"• Pontos ACIMA: Problemas em {indices_importantes_c2[0][0].replace('Indice_', '')}")
print(f"• Pontos ABAIXO: Melhores em {indices_importantes_c2[0][0].replace('Indice_', '')}")

# 4. Criar um "mapa de interpretação"
print(f"\n=== MAPA DE INTERPRETAÇÃO DO GRÁFICO ===")
print(f"┌─────────────────┬─────────────────┐")
print(f"│  QUADRANTE II   │  QUADRANTE I    │")
print(f"│ (X-, Y+)        │ (X+, Y+)        │")
print(f"│ Baixo {indices_importantes_c1[0][0].replace('Indice_', '')[:8]}│ Alto {indices_importantes_c1[0][0].replace('Indice_', '')[:8]} │")
print(f"│ Alto {indices_importantes_c2[0][0].replace('Indice_', '')[:8]} │ Alto {indices_importantes_c2[0][0].replace('Indice_', '')[:8]} │")
print(f"├─────────────────┼─────────────────┤")
print(f"│  QUADRANTE III  │  QUADRANTE IV   │")
print(f"│ (X-, Y-)        │ (X+, Y-)        │")
print(f"│ Baixo {indices_importantes_c1[0][0].replace('Indice_', '')[:8]}│ Alto {indices_importantes_c1[0][0].replace('Indice_', '')[:8]} │")
print(f"│ Baixo {indices_importantes_c2[0][0].replace('Indice_', '')[:8]}│ Baixo {indices_importantes_c2[0][0].replace('Indice_', '')[:8]}│")
print(f"└─────────────────┴─────────────────┘")

# === Análise final ===
print(f"\n=== RESULTADO FINAL ===")
n_clusters = len(set(final_labels)) - (1 if -1 in final_labels else 0)
n_noise = list(final_labels).count(-1) if -1 in final_labels else 0
noise_pct = n_noise / len(final_labels) * 100

print(f"Algoritmo usado: {best_algorithm}")
print(f"Número de clusters: {n_clusters}")
print(f"Pontos de ruído: {n_noise} ({noise_pct:.1f}%)")
print(f"Silhouette Score: {algorithms_scores[best_algorithm]:.3f}")

# Distribuição dos clusters
cluster_counts = Counter(final_labels)
print(f"\nDistribuição dos clusters:")
for cluster in sorted(cluster_counts.keys()):
    count = cluster_counts[cluster]
    pct = count / len(final_labels) * 100
    if cluster == -1:
        print(f"  Ruído: {count} IDs únicos ({pct:.1f}%)")
    else:
        print(f"  Cluster {cluster}: {count} IDs únicos ({pct:.1f}%)")

# === FUNÇÃO PARA GERAR NOMES DINÂMICOS DOS CLUSTERS ===
def gerar_nomes_clusters_dinamicos(df_result, final_labels, filtered_cols):
    """
    Gera nomes e descrições dinâmicas para os clusters baseado nos dados reais
    """
    cluster_info = {}
    
    # Calcular médias globais para comparação
    global_means = df_result[filtered_cols].mean()
    
    unique_clusters = sorted(set(final_labels))
    
    # Analisar cada cluster
    for cluster_id in unique_clusters:
        if cluster_id == -1:
            cluster_info[cluster_id] = {
                'nome': 'OUTLIERS',
                'descricao': 'IDs únicos com padrões atípicos - Revisar individualmente',
                'prioridade': 'ALTA'
            }
            continue
            
        # Dados do cluster
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        cluster_means = cluster_data[filtered_cols].mean()
        
        # Comparar com média global
        desvios = cluster_means - global_means
        
        # Identificar características principais
        piores_indices = desvios.nlargest(3)  # Índices mais altos (piores)
        melhores_indices = desvios.nsmallest(3)  # Índices mais baixos (melhores)
        
        # Calcular score geral do cluster
        score_geral = cluster_means.mean()
        
        # Determinar categoria baseada no score
        if score_geral >= 7:
            categoria = 'CRÍTICO'
            prioridade = 'ALTA'
        elif score_geral >= 5:
            categoria = 'PROBLEMÁTICO'
            prioridade = 'MÉDIA-ALTA'
        elif score_geral >= 3:
            categoria = 'MODERADO'
            prioridade = 'MÉDIA'
        else:
            categoria = 'BOM'
            prioridade = 'BAIXA'
        
        # Identificar problema principal
        problema_principal = piores_indices.index[0].replace('Indice_', '').replace('_', ' ')
        
        # Criar nome descritivo
        nome = f"{categoria} - {problema_principal}"
        
        # Criar descrição detalhada
        descricao = f"Score médio: {score_geral:.2f} | "
        descricao += f"Principais problemas: {', '.join([idx.replace('Indice_', '') for idx in piores_indices.index[:2]])}"
        
        cluster_info[cluster_id] = {
            'nome': nome,
            'descricao': descricao,
            'prioridade': prioridade,
            'score': score_geral
        }
    
    return cluster_info

# === Análise dinâmica dos clusters ===
if n_clusters > 0 and n_clusters < 20:
    print(f"\n=== ANÁLISE DINÂMICA DOS CLUSTERS ===")
    
    # Gerar informações dinâmicas
    cluster_info_dinamico = gerar_nomes_clusters_dinamicos(df_result, final_labels, filtered_cols)
    
    for cluster_id in sorted(set(final_labels)):
        info = cluster_info_dinamico[cluster_id]
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        
        print(f"\n--- CLUSTER {cluster_id}: {info['nome']} ---")
        print(f"IDs únicos: {len(cluster_data)}")
        print(f"Prioridade: {info['prioridade']}")
        print(f"Descrição: {info['descricao']}")
        
        if cluster_id != -1:
            # Mostrar médias dos principais índices
            cluster_means = cluster_data[filtered_cols].mean()
            print("Top 3 índices mais problemáticos:")
            for idx, valor in cluster_means.nlargest(3).items():
                print(f"  {idx.replace('Indice_', '')}: {valor:.2f}")

# === Preparação do resultado final DINÂMICO ===
print(f"\n=== PREPARANDO RESULTADO FINAL DINÂMICO ===")

# Usar a chave única criada
id_unico_col = 'ID_UNICO'
print(f"Usando chave única: {id_unico_col}")

# Criar DataFrame de mapeamento cluster
cluster_mapping = df_result[[id_unico_col, 'cluster']].copy()

# Fazer merge com DataFrame original
df_original_completo = df.copy()
# Garantir que não tenha IDs únicos duplicados
cluster_mapping = cluster_mapping.drop_duplicates(subset=id_unico_col)

# Garantir que o tipo é o mesmo
cluster_mapping[id_unico_col] = cluster_mapping[id_unico_col].astype(df_original_completo[id_unico_col].dtype)

resultado_completo = df_original_completo.merge(
    cluster_mapping, 
    on=id_unico_col, 
    how='left'
)

# Preencher NaN nos clusters
resultado_completo['cluster'] = resultado_completo['cluster'].fillna(-999)

# === APLICAR INFORMAÇÕES DINÂMICAS ===
def mapear_cluster_info(cluster_id):
    if cluster_id == -999:
        return "REMOVIDO_LIMPEZA - Outlier extremo removido na pré-análise"
    elif cluster_id in cluster_info_dinamico:
        info = cluster_info_dinamico[cluster_id]
        return f"{info['nome']} - Prioridade {info['prioridade']}"
    else:
        return "DESCONHECIDO"

# Aplicar mapeamento dinâmico
resultado_completo['cluster_nome'] = resultado_completo['cluster'].apply(mapear_cluster_info)

# Reorganizar colunas - colocar as colunas identificadoras primeiro
colunas_identificadoras = [id_unico_col, 'Freq', 'Chave_Site', 'Setor', 'cluster', 'cluster_nome']
outras_colunas = [col for col in resultado_completo.columns if col not in colunas_identificadoras]
colunas_ordenadas = colunas_identificadoras + outras_colunas

resultado_final_completo = resultado_completo[colunas_ordenadas].copy()

# === Salvar resultado ===
timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
filename = f'clustering_resultado_ID_UNICO_{timestamp}.csv'

resultado_final_completo.to_csv(filename, index=False, sep=';', encoding='utf-8')

print(f"✅ Arquivo salvo: '{filename}'")
print(f"📊 Colunas no arquivo: {len(resultado_final_completo.columns)}")
print(f"📋 Linhas no arquivo: {len(resultado_final_completo)}")

# === Estatísticas do resultado final ===
print(f"\n=== ESTATÍSTICAS FINAIS DINÂMICAS ===")
cluster_stats = resultado_final_completo['cluster'].value_counts().sort_index()

for cluster_id, count in cluster_stats.items():
    pct = count / len(resultado_final_completo) * 100
    nome = mapear_cluster_info(cluster_id)
    print(f"Cluster {cluster_id}: {count:,} IDs únicos ({pct:.1f}%) - {nome}")

# === ANÁLISE POR COMPONENTES DA CHAVE ÚNICA ===
print(f"\n=== ANÁLISE POR COMPONENTES DA CHAVE ÚNICA ===")

# Análise por Frequência
print("\n📡 Distribuição por FREQUÊNCIA:")
freq_analysis = resultado_final_completo.groupby('Freq')['cluster'].value_counts().unstack(fill_value=0)
for freq in freq_analysis.index:
    total = freq_analysis.loc[freq].sum()
    print(f"  Freq {freq}: {total} IDs únicos")
    # Mostrar top 2 clusters para cada frequência
    top_clusters = freq_analysis.loc[freq].nlargest(2)
    for cluster_id, count in top_clusters.items():
        if count > 0:
            pct = count / total * 100
            nome = mapear_cluster_info(cluster_id)
            print(f"    Cluster {cluster_id}: {count} ({pct:.1f}%) - {nome[:30]}...")

# Análise por Site
print("\n🏗️ Distribuição por SITE (Top 10):")
site_analysis = resultado_final_completo.groupby('Chave_Site').agg({
    'cluster': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else -999,
    'ID_UNICO': 'count'
}).rename(columns={'ID_UNICO': 'total_ids'}).sort_values('total_ids', ascending=False)

for site, row in site_analysis.head(10).iterrows():
    cluster_predominante = row['cluster']
    total = row['total_ids']
    nome_cluster = mapear_cluster_info(cluster_predominante)
    print(f"  Site {site}: {total} IDs únicos - Cluster predominante: {cluster_predominante} ({nome_cluster[:30]}...)")

# Análise por Setor
print("\n📶 Distribuição por SETOR:")
setor_analysis = resultado_final_completo.groupby('Setor')['cluster'].value_counts().unstack(fill_value=0)
for setor in sorted(setor_analysis.index):
    total = setor_analysis.loc[setor].sum()
    print(f"  Setor {setor}: {total} IDs únicos")
    # Mostrar cluster predominante
    cluster_predominante = setor_analysis.loc[setor].idxmax()
    count_predominante = setor_analysis.loc[setor].max()
    pct = count_predominante / total * 100
    nome = mapear_cluster_info(cluster_predominante)
    print(f"    Cluster predominante: {cluster_predominante} ({count_predominante} - {pct:.1f}%) - {nome[:40]}...")

# === Visualização ===
plt.figure(figsize=(15, 10))

if n_clusters <= 20:
    unique_clusters = sorted(set(final_labels))
    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_clusters)))
    
    for i, cluster_id in enumerate(unique_clusters):
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        if cluster_id == -1:
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'], 
                       c='red', marker='x', s=10, alpha=0.6, 
                       label=f'Ruído ({len(cluster_data)})')
        else:
            nome_cluster = cluster_info_dinamico[cluster_id]['nome'][:20]  # Truncar para visualização
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'],
                       c=[colors[i]], s=15, alpha=0.7, 
                       label=f'C{cluster_id}: {nome_cluster} ({len(cluster_data)})')
    
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering por ID_ÚNICO - {best_algorithm} | Score: {algorithms_scores[best_algorithm]:.3f}')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
else:
    plt.scatter(df_result['pca_1'], df_result['pca_2'], 
               c=final_labels, cmap='tab20', s=30, alpha=0.7)
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering por ID_ÚNICO - {best_algorithm} ({n_clusters} clusters)')
    plt.colorbar()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# === RELATÓRIO DE QUALIDADE DOS DADOS ===
print(f"\n=== RELATÓRIO DE QUALIDADE DOS DADOS ===")
print(f"📋 Total de registros originais: {len(df):,}")
print(f"🔑 IDs únicos criados: {df['ID_UNICO'].nunique():,}")
print(f"🧹 IDs únicos após limpeza: {len(df_clean):,}")
print(f"📊 IDs únicos com cluster: {len(df_result):,}")
print(f"📁 IDs únicos no arquivo final: {len(resultado_final_completo):,}")

duplicatas_freq = df.groupby(['Freq', 'Chave_Site']).size()
sites_com_duplicatas = (duplicatas_freq > 3).sum()  # Sites com mais de 3 setores
print(f"🏗️ Sites únicos: {df['Chave_Site'].nunique():,}")
print(f"📡 Frequências únicas: {df['Freq'].nunique()}")
print(f"📶 Setores únicos: {df['Setor'].nunique()}")
print(f"⚠️ Sites com mais de 3 setores: {sites_com_duplicatas}")

# === RECOMENDAÇÕES FINAIS ===
print(f"\n=== RECOMENDAÇÕES FINAIS ===")
print(f"✅ DADOS PROCESSADOS COM SUCESSO!")
print(f"🔑 Chave única ID_UNICO criada: Freq + Chave_Site + Setor")
print(f"📊 {n_clusters} clusters identificados dinamicamente")
print(f"🎯 Priorize clusters com classificação 'CRÍTICO' e 'PROBLEMÁTICO'")
print(f"📁 Arquivo salvo: '{filename}' - Pronto para análise!")

print(f"\n🔍 PRÓXIMOS PASSOS SUGERIDOS:")
print(f"1. Analise os clusters CRÍTICOS primeiro (prioridade ALTA)")
print(f"2. Investigue outliers (-1) individualmente")
print(f"3. Compare performance entre sites da mesma frequência")
print(f"4. Monitore evolução temporal dos clusters")
print(f"5. Use ID_UNICO para rastreamento individual")

print(f"\n🎉 ANÁLISE COMPLETA COM CHAVE ÚNICA!")
print(f"📄 Arquivo final: '{filename}'")
print(f"🤖 Algoritmo: {best_algorithm}")
print(f"📊 Score: {algorithms_scores[best_algorithm]:.3f}")
print(f"🔢 Clusters identificados: {n_clusters}")
print(f"🔑 IDs únicos processados: {len(resultado_final_completo):,}")
print(f"📈 Todos os clusters são nomeados DINAMICAMENTE baseados nos dados reais!")
print(f"🔄 Código adaptado para usar chave única Freq+Chave_Site+Setor!")
