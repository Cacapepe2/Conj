import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.feature_selection import VarianceThreshold
import hdbscan
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# === Leitura do CSV ===
df = pd.read_csv('INFO-Sumario_Taxa_Setor-2025-08-06.csv', encoding='ISO-8859-1', sep=';')

# === CRIAÃ‡ÃƒO DA CHAVE ÃšNICA ===
print(f"=== CRIANDO CHAVE ÃšNICA ===")

# Verificar se as colunas necessÃ¡rias existem
required_cols = ['Freq', 'Chave_Site', 'Setor']
missing_cols = [col for col in required_cols if col not in df.columns]

if missing_cols:
    print(f"âŒ ERRO: Colunas obrigatÃ³rias nÃ£o encontradas: {missing_cols}")
    print(f"Colunas disponÃ­veis: {list(df.columns)}")
    exit()

# Criar chave Ãºnica
df['ID_UNICO'] = df['Freq'].astype(str) + '_' + df['Chave_Site'].astype(str) + '_' + df['Setor'].astype(str)


print(f"âœ… Chave Ãºnica criada: ID_UNICO")
print(f"Exemplo de IDs Ãºnicos:")
for i, id_unico in enumerate(df['ID_UNICO'].head(5)):
    print(f"  {i+1}. {id_unico}")

# Verificar duplicatas
duplicatas = df['ID_UNICO'].duplicated().sum()
if duplicatas > 0:
    print(f"âš ï¸  AVISO: Encontradas {duplicatas} duplicatas na chave Ãºnica!")
    print("Removendo duplicatas (mantendo primeira ocorrÃªncia)...")
    df = df.drop_duplicates(subset='ID_UNICO', keep='first')
    print(f"Dados apÃ³s remoÃ§Ã£o de duplicatas: {len(df)} registros")
else:
    print(f"âœ… Nenhuma duplicata encontrada - {len(df)} registros Ãºnicos")

# === Colunas de interesse ===
indices_cols = [
    'Indice_Taxa', 'Indice_Indisp', 'TX_Indice', 'Indice_TX_PktLoss',
    'Indice_TX_Delay', 'PRB_Indice', 'UE_Indice', 'Indice_RRC',
    'Indice_ERAB', 'Indice_CSFB', 'Indice_SIGS1', 'Indice_QUEDA.SESSAO',
    'Indice_ERAB.QCI1', 'Indice_DROP.RATE.QCI1', 'Indice_INTERFERÃŠNCIA',
    'Indice_HO.VOLTE', 'Indice_HANDOVER', 'Indice_CQI', 'Indice_PACKET.DELAY',
    'Indice_UTIL.PDCCH', 'Indice_LATENCIA.BUFFER.DL',
    'Indice_REASSEMBLY.FAIL.RATE', 'Indice_FRAGMENTACAO.TX',
    'Indice_DESVIO.CLOCK'
]

# === ConversÃ£o dos dados ===
for col in indices_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.').str.strip(), errors='coerce')

# Filtrar apenas colunas que existem no DataFrame
indices_cols = [col for col in indices_cols if col in df.columns]

print(f"\n=== DIAGNÃ“STICO DOS DADOS ===")
print(f"Shape original: {df.shape}")
print(f"Colunas de Ã­ndices encontradas: {len(indices_cols)}")
print(f"IDs Ãºnicos: {df['ID_UNICO'].nunique()}")

# === AnÃ¡lise de missings ===
missing_analysis = df[indices_cols].isnull().sum()
print(f"\nValores faltantes por coluna:")
for col, missing in missing_analysis.items():
    if missing > 0:
        pct = missing / len(df) * 100
        print(f"  {col}: {missing} ({pct:.1f}%)")

# === Remover colunas com muitos missings ===
cols_to_keep = []
for col in indices_cols:
    missing_pct = df[col].isnull().sum() / len(df)
    if missing_pct < 0.5:  # Manter colunas com menos de 50% missing
        cols_to_keep.append(col)
    else:
        print(f"Removendo {col} - muitos valores faltantes ({missing_pct*100:.1f}%)")

indices_cols = cols_to_keep
print(f"Colunas mantidas: {len(indices_cols)}")

if len(indices_cols) == 0:
    print("âŒ ERRO: Nenhuma coluna de Ã­ndices vÃ¡lida encontrada!")
    exit()

# === AnÃ¡lise de variÃ¢ncia ===
df_temp = df[indices_cols].fillna(df[indices_cols].median())
variance_selector = VarianceThreshold(threshold=0.01)
variance_selector.fit(df_temp)

low_variance_cols = [col for i, col in enumerate(indices_cols) 
                    if not variance_selector.get_support()[i]]
if low_variance_cols:
    print(f"Removendo colunas com baixa variÃ¢ncia: {low_variance_cols}")
    indices_cols = [col for col in indices_cols if col not in low_variance_cols]

# === Remover outliers extremos ANTES do clustering ===
print(f"\n=== TRATAMENTO DE OUTLIERS ===")
df_clean = df.copy()

# Para cada Ã­ndice, remover valores impossÃ­veis (fora de 0-10)
for col in indices_cols:
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= 0) & (df_clean[col] <= 10))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} valores fora do range 0-10")

# Remover outliers usando IQR para cada coluna
for col in indices_cols:
    Q1 = df_clean[col].quantile(0.05)
    Q3 = df_clean[col].quantile(0.95)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    before = len(df_clean)
    df_clean = df_clean[(df_clean[col].isna()) | 
                       ((df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound))]
    after = len(df_clean)
    if before != after:
        print(f"{col}: removidos {before-after} outliers extremos")

print(f"Dados apÃ³s limpeza: {len(df_clean)} IDs Ãºnicos ({len(df_clean)/len(df)*100:.1f}% do original)")

# === RemoÃ§Ã£o de correlaÃ§Ã£o alta ===
corr_matrix = df_clean[indices_cols].corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]
filtered_cols = [col for col in indices_cols if col not in to_drop]

if to_drop:
    print(f"\nColunas removidas por alta correlaÃ§Ã£o (>0.8): {to_drop}")
print(f"Colunas finais para clustering: {len(filtered_cols)}")

# === PreparaÃ§Ã£o final dos dados ===
X_final = df_clean[filtered_cols].fillna(df_clean[filtered_cols].median())
print(f"Dados finais para clustering: {X_final.shape}")

# === Teste com mÃºltiplos algoritmos ===
print(f"\n=== TESTANDO DIFERENTES ALGORITMOS ===")

# Preparar pipeline de prÃ©-processamento
preprocessor = Pipeline([
    ("scaler", StandardScaler())
])

X_scaled = preprocessor.fit_transform(X_final)

# 1. KMeans com diferentes nÃºmeros de clusters
print("\n1. TESTANDO K-MEANS:")
silhouette_scores = []
k_range = range(3, 15)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    ch_score = calinski_harabasz_score(X_scaled, labels)
    silhouette_scores.append(score)
    print(f"  K={k}: Silhouette={score:.3f}, CH_Score={ch_score:.1f}")

# Melhor K para KMeans
best_k = k_range[np.argmax(silhouette_scores)]
print(f"  Melhor K para K-Means: {best_k} (Score: {max(silhouette_scores):.3f})")

# 2. DBSCAN
print("\n2. TESTANDO DBSCAN:")
eps_values = [0.3, 0.5, 0.7, 1.0, 1.5]
min_samples_values = [5, 10, 20]

best_dbscan_score = -1
best_dbscan_params = None

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        noise_pct = n_noise / len(labels) * 100
        
        if n_clusters > 1 and noise_pct < 30:
            try:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  eps={eps}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, RuÃ­do={noise_pct:.1f}%")
                
                if score > best_dbscan_score:
                    best_dbscan_score = score
                    best_dbscan_params = (eps, min_samples)
            except:
                continue

# 3. HDBSCAN mais conservador
print("\n3. TESTANDO HDBSCAN (CONSERVADOR):")
min_cluster_sizes = [50, 100, 200]
min_samples_values = [5, 10, 20]

best_hdbscan_score = -1
best_hdbscan_params = None

for min_size in min_cluster_sizes:
    for min_samples in min_samples_values:
        try:
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_size,
                min_samples=min_samples,
                cluster_selection_epsilon=0.0
            )
            labels = clusterer.fit_predict(X_scaled)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            noise_pct = n_noise / len(labels) * 100
            
            if n_clusters > 1 and noise_pct < 30:
                if -1 in labels:
                    mask = labels != -1
                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:
                        score = silhouette_score(X_scaled[mask], labels[mask])
                    else:
                        score = -1
                else:
                    score = silhouette_score(X_scaled, labels)
                
                print(f"  min_size={min_size}, min_samples={min_samples}: Score={score:.3f}, "
                      f"Clusters={n_clusters}, RuÃ­do={noise_pct:.1f}%")
                
                if score > best_hdbscan_score:
                    best_hdbscan_score = score
                    best_hdbscan_params = (min_size, min_samples)
        except:
            continue

# === Escolher melhor algoritmo ===
algorithms_scores = {
    'KMeans': max(silhouette_scores),
    'DBSCAN': best_dbscan_score if best_dbscan_params else -1,
    'HDBSCAN': best_hdbscan_score if best_hdbscan_params else -1
}

print(f"\n=== COMPARAÃ‡ÃƒO FINAL ===")
for algo, score in algorithms_scores.items():
    print(f"{algo}: {score:.3f}")

best_algorithm = max(algorithms_scores, key=algorithms_scores.get)
print(f"Melhor algoritmo: {best_algorithm}")

# === Aplicar melhor algoritmo ===
if best_algorithm == 'KMeans':
    final_clusterer = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando K-Means com K={best_k}")

elif best_algorithm == 'DBSCAN':
    eps, min_samples = best_dbscan_params
    final_clusterer = DBSCAN(eps=eps, min_samples=min_samples)
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando DBSCAN com eps={eps}, min_samples={min_samples}")

else:  # HDBSCAN
    min_size, min_samples = best_hdbscan_params
    final_clusterer = hdbscan.HDBSCAN(
        min_cluster_size=min_size,
        min_samples=min_samples,
        cluster_selection_epsilon=0.0
    )
    final_labels = final_clusterer.fit_predict(X_scaled)
    print(f"Usando HDBSCAN com min_size={min_size}, min_samples={min_samples}")

# === Adicionar resultados ao DataFrame ===
df_result = df_clean.copy()
df_result['cluster'] = final_labels

# === PCA para visualizaÃ§Ã£o ===
pca_viz = PCA(n_components=2)
X_pca_viz = pca_viz.fit_transform(X_scaled)
df_result['pca_1'] = X_pca_viz[:, 0]
df_result['pca_2'] = X_pca_viz[:, 1]

# === ANÃLISE DETALHADA DO PCA ===
print(f"\n=== ANÃLISE DETALHADA DO PCA ===")

# 1. VariÃ¢ncia explicada por cada componente
variancia_explicada = pca_viz.explained_variance_ratio_
print(f"PCA Component 1 (Eixo X): explica {variancia_explicada[0]*100:.1f}% da variaÃ§Ã£o")
print(f"PCA Component 2 (Eixo Y): explica {variancia_explicada[1]*100:.1f}% da variaÃ§Ã£o")
print(f"Total explicado: {sum(variancia_explicada)*100:.1f}% da variaÃ§Ã£o original")

# 2. Quais Ã­ndices mais influenciam cada componente
componentes = pca_viz.components_  # Shape: (2, n_features)

print(f"\n--- PCA COMPONENT 1 (Eixo X) ---")
print("Ãndices que MAIS influenciam este eixo:")
# Pegar os 5 Ã­ndices com maior peso (positivo ou negativo)
component1_weights = componentes[0]
indices_importantes_c1 = []
for i, peso in enumerate(component1_weights):
    indices_importantes_c1.append((filtered_cols[i], peso))

# Ordenar por valor absoluto (importÃ¢ncia)
indices_importantes_c1.sort(key=lambda x: abs(x[1]), reverse=True)

for indice, peso in indices_importantes_c1[:5]:
    sinal = "â†‘" if peso > 0 else "â†“"
    print(f"  {indice}: {peso:.3f} {sinal}")

print(f"\n--- PCA COMPONENT 2 (Eixo Y) ---")
print("Ãndices que MAIS influenciam este eixo:")
component2_weights = componentes[1]
indices_importantes_c2 = []
for i, peso in enumerate(component2_weights):
    indices_importantes_c2.append((filtered_cols[i], peso))

indices_importantes_c2.sort(key=lambda x: abs(x[1]), reverse=True)

for indice, peso in indices_importantes_c2[:5]:
    sinal = "â†‘" if peso > 0 else "â†“"
    print(f"  {indice}: {peso:.3f} {sinal}")

# 3. InterpretaÃ§Ã£o prÃ¡tica
print(f"\n=== INTERPRETAÃ‡ÃƒO PRÃTICA ===")
print("ğŸ” COMO LER O GRÃFICO:")
print(f"ğŸ“ˆ Eixo X (PCA 1): Representa principalmente '{indices_importantes_c1[0][0].replace('Indice_', '')}'")
print(f"ğŸ“Š Eixo Y (PCA 2): Representa principalmente '{indices_importantes_c2[0][0].replace('Indice_', '')}'")

print(f"\nğŸ¯ SIGNIFICADO DAS POSIÃ‡Ã•ES:")
print(f"â€¢ Pontos Ã  DIREITA: Problemas em {indices_importantes_c1[0][0].replace('Indice_', '')}")
print(f"â€¢ Pontos Ã  ESQUERDA: Melhores em {indices_importantes_c1[0][0].replace('Indice_', '')}")
print(f"â€¢ Pontos ACIMA: Problemas em {indices_importantes_c2[0][0].replace('Indice_', '')}")
print(f"â€¢ Pontos ABAIXO: Melhores em {indices_importantes_c2[0][0].replace('Indice_', '')}")

# 4. Criar um "mapa de interpretaÃ§Ã£o"
print(f"\n=== MAPA DE INTERPRETAÃ‡ÃƒO DO GRÃFICO ===")
print(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"â”‚  QUADRANTE II   â”‚  QUADRANTE I    â”‚")
print(f"â”‚ (X-, Y+)        â”‚ (X+, Y+)        â”‚")
print(f"â”‚ Baixo {indices_importantes_c1[0][0].replace('Indice_', '')[:8]}â”‚ Alto {indices_importantes_c1[0][0].replace('Indice_', '')[:8]} â”‚")
print(f"â”‚ Alto {indices_importantes_c2[0][0].replace('Indice_', '')[:8]} â”‚ Alto {indices_importantes_c2[0][0].replace('Indice_', '')[:8]} â”‚")
print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print(f"â”‚  QUADRANTE III  â”‚  QUADRANTE IV   â”‚")
print(f"â”‚ (X-, Y-)        â”‚ (X+, Y-)        â”‚")
print(f"â”‚ Baixo {indices_importantes_c1[0][0].replace('Indice_', '')[:8]}â”‚ Alto {indices_importantes_c1[0][0].replace('Indice_', '')[:8]} â”‚")
print(f"â”‚ Baixo {indices_importantes_c2[0][0].replace('Indice_', '')[:8]}â”‚ Baixo {indices_importantes_c2[0][0].replace('Indice_', '')[:8]}â”‚")
print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# === AnÃ¡lise final ===
print(f"\n=== RESULTADO FINAL ===")
n_clusters = len(set(final_labels)) - (1 if -1 in final_labels else 0)
n_noise = list(final_labels).count(-1) if -1 in final_labels else 0
noise_pct = n_noise / len(final_labels) * 100

print(f"Algoritmo usado: {best_algorithm}")
print(f"NÃºmero de clusters: {n_clusters}")
print(f"Pontos de ruÃ­do: {n_noise} ({noise_pct:.1f}%)")
print(f"Silhouette Score: {algorithms_scores[best_algorithm]:.3f}")

# DistribuiÃ§Ã£o dos clusters
cluster_counts = Counter(final_labels)
print(f"\nDistribuiÃ§Ã£o dos clusters:")
for cluster in sorted(cluster_counts.keys()):
    count = cluster_counts[cluster]
    pct = count / len(final_labels) * 100
    if cluster == -1:
        print(f"  RuÃ­do: {count} IDs Ãºnicos ({pct:.1f}%)")
    else:
        print(f"  Cluster {cluster}: {count} IDs Ãºnicos ({pct:.1f}%)")

# === FUNÃ‡ÃƒO PARA GERAR NOMES DINÃ‚MICOS DOS CLUSTERS ===
def gerar_nomes_clusters_dinamicos(df_result, final_labels, filtered_cols):
    """
    Gera nomes e descriÃ§Ãµes dinÃ¢micas para os clusters baseado nos dados reais
    """
    cluster_info = {}
    
    # Calcular mÃ©dias globais para comparaÃ§Ã£o
    global_means = df_result[filtered_cols].mean()
    
    unique_clusters = sorted(set(final_labels))
    
    # Analisar cada cluster
    for cluster_id in unique_clusters:
        if cluster_id == -1:
            cluster_info[cluster_id] = {
                'nome': 'OUTLIERS',
                'descricao': 'IDs Ãºnicos com padrÃµes atÃ­picos - Revisar individualmente',
                'prioridade': 'ALTA'
            }
            continue
            
        # Dados do cluster
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        cluster_means = cluster_data[filtered_cols].mean()
        
        # Comparar com mÃ©dia global
        desvios = cluster_means - global_means
        
        # Identificar caracterÃ­sticas principais
        piores_indices = desvios.nlargest(3)  # Ãndices mais altos (piores)
        melhores_indices = desvios.nsmallest(3)  # Ãndices mais baixos (melhores)
        
        # Calcular score geral do cluster
        score_geral = cluster_means.mean()
        
        # Determinar categoria baseada no score
        if score_geral >= 7:
            categoria = 'CRÃTICO'
            prioridade = 'ALTA'
        elif score_geral >= 5:
            categoria = 'PROBLEMÃTICO'
            prioridade = 'MÃ‰DIA-ALTA'
        elif score_geral >= 3:
            categoria = 'MODERADO'
            prioridade = 'MÃ‰DIA'
        else:
            categoria = 'BOM'
            prioridade = 'BAIXA'
        
        # Identificar problema principal
        problema_principal = piores_indices.index[0].replace('Indice_', '').replace('_', ' ')
        
        # Criar nome descritivo
        nome = f"{categoria} - {problema_principal}"
        
        # Criar descriÃ§Ã£o detalhada
        descricao = f"Score mÃ©dio: {score_geral:.2f} | "
        descricao += f"Principais problemas: {', '.join([idx.replace('Indice_', '') for idx in piores_indices.index[:2]])}"
        
        cluster_info[cluster_id] = {
            'nome': nome,
            'descricao': descricao,
            'prioridade': prioridade,
            'score': score_geral
        }
    
    return cluster_info

# === AnÃ¡lise dinÃ¢mica dos clusters ===
if n_clusters > 0 and n_clusters < 20:
    print(f"\n=== ANÃLISE DINÃ‚MICA DOS CLUSTERS ===")
    
    # Gerar informaÃ§Ãµes dinÃ¢micas
    cluster_info_dinamico = gerar_nomes_clusters_dinamicos(df_result, final_labels, filtered_cols)
    
    for cluster_id in sorted(set(final_labels)):
        info = cluster_info_dinamico[cluster_id]
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        
        print(f"\n--- CLUSTER {cluster_id}: {info['nome']} ---")
        print(f"IDs Ãºnicos: {len(cluster_data)}")
        print(f"Prioridade: {info['prioridade']}")
        print(f"DescriÃ§Ã£o: {info['descricao']}")
        
        if cluster_id != -1:
            # Mostrar mÃ©dias dos principais Ã­ndices
            cluster_means = cluster_data[filtered_cols].mean()
            print("Top 3 Ã­ndices mais problemÃ¡ticos:")
            for idx, valor in cluster_means.nlargest(3).items():
                print(f"  {idx.replace('Indice_', '')}: {valor:.2f}")

# === PreparaÃ§Ã£o do resultado final DINÃ‚MICO ===
print(f"\n=== PREPARANDO RESULTADO FINAL DINÃ‚MICO ===")

# Usar a chave Ãºnica criada
id_unico_col = 'ID_UNICO'
print(f"Usando chave Ãºnica: {id_unico_col}")

# Criar DataFrame de mapeamento cluster
cluster_mapping = df_result[[id_unico_col, 'cluster']].copy()

# Fazer merge com DataFrame original
df_original_completo = df.copy()
# Garantir que nÃ£o tenha IDs Ãºnicos duplicados
cluster_mapping = cluster_mapping.drop_duplicates(subset=id_unico_col)

# Garantir que o tipo Ã© o mesmo
cluster_mapping[id_unico_col] = cluster_mapping[id_unico_col].astype(df_original_completo[id_unico_col].dtype)

resultado_completo = df_original_completo.merge(
    cluster_mapping, 
    on=id_unico_col, 
    how='left'
)

# Preencher NaN nos clusters
resultado_completo['cluster'] = resultado_completo['cluster'].fillna(-999)

# === APLICAR INFORMAÃ‡Ã•ES DINÃ‚MICAS ===
def mapear_cluster_info(cluster_id):
    if cluster_id == -999:
        return "REMOVIDO_LIMPEZA - Outlier extremo removido na prÃ©-anÃ¡lise"
    elif cluster_id in cluster_info_dinamico:
        info = cluster_info_dinamico[cluster_id]
        return f"{info['nome']} - Prioridade {info['prioridade']}"
    else:
        return "DESCONHECIDO"

# Aplicar mapeamento dinÃ¢mico
resultado_completo['cluster_nome'] = resultado_completo['cluster'].apply(mapear_cluster_info)

# Reorganizar colunas - colocar as colunas identificadoras primeiro
colunas_identificadoras = [id_unico_col, 'Freq', 'Chave_Site', 'Setor', 'cluster', 'cluster_nome']
outras_colunas = [col for col in resultado_completo.columns if col not in colunas_identificadoras]
colunas_ordenadas = colunas_identificadoras + outras_colunas

resultado_final_completo = resultado_completo[colunas_ordenadas].copy()

# === Salvar resultado ===
timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
filename = f'clustering_resultado_ID_UNICO_{timestamp}.csv'

resultado_final_completo.to_csv(filename, index=False, sep=';', encoding='utf-8')

print(f"âœ… Arquivo salvo: '{filename}'")
print(f"ğŸ“Š Colunas no arquivo: {len(resultado_final_completo.columns)}")
print(f"ğŸ“‹ Linhas no arquivo: {len(resultado_final_completo)}")

# === EstatÃ­sticas do resultado final ===
print(f"\n=== ESTATÃSTICAS FINAIS DINÃ‚MICAS ===")
cluster_stats = resultado_final_completo['cluster'].value_counts().sort_index()

for cluster_id, count in cluster_stats.items():
    pct = count / len(resultado_final_completo) * 100
    nome = mapear_cluster_info(cluster_id)
    print(f"Cluster {cluster_id}: {count:,} IDs Ãºnicos ({pct:.1f}%) - {nome}")

# === ANÃLISE POR COMPONENTES DA CHAVE ÃšNICA ===
print(f"\n=== ANÃLISE POR COMPONENTES DA CHAVE ÃšNICA ===")

# AnÃ¡lise por FrequÃªncia
print("\nğŸ“¡ DistribuiÃ§Ã£o por FREQUÃŠNCIA:")
freq_analysis = resultado_final_completo.groupby('Freq')['cluster'].value_counts().unstack(fill_value=0)
for freq in freq_analysis.index:
    total = freq_analysis.loc[freq].sum()
    print(f"  Freq {freq}: {total} IDs Ãºnicos")
    # Mostrar top 2 clusters para cada frequÃªncia
    top_clusters = freq_analysis.loc[freq].nlargest(2)
    for cluster_id, count in top_clusters.items():
        if count > 0:
            pct = count / total * 100
            nome = mapear_cluster_info(cluster_id)
            print(f"    Cluster {cluster_id}: {count} ({pct:.1f}%) - {nome[:30]}...")

# AnÃ¡lise por Site
print("\nğŸ—ï¸ DistribuiÃ§Ã£o por SITE (Top 10):")
site_analysis = resultado_final_completo.groupby('Chave_Site').agg({
    'cluster': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else -999,
    'ID_UNICO': 'count'
}).rename(columns={'ID_UNICO': 'total_ids'}).sort_values('total_ids', ascending=False)

for site, row in site_analysis.head(10).iterrows():
    cluster_predominante = row['cluster']
    total = row['total_ids']
    nome_cluster = mapear_cluster_info(cluster_predominante)
    print(f"  Site {site}: {total} IDs Ãºnicos - Cluster predominante: {cluster_predominante} ({nome_cluster[:30]}...)")

# AnÃ¡lise por Setor
print("\nğŸ“¶ DistribuiÃ§Ã£o por SETOR:")
setor_analysis = resultado_final_completo.groupby('Setor')['cluster'].value_counts().unstack(fill_value=0)
for setor in sorted(setor_analysis.index):
    total = setor_analysis.loc[setor].sum()
    print(f"  Setor {setor}: {total} IDs Ãºnicos")
    # Mostrar cluster predominante
    cluster_predominante = setor_analysis.loc[setor].idxmax()
    count_predominante = setor_analysis.loc[setor].max()
    pct = count_predominante / total * 100
    nome = mapear_cluster_info(cluster_predominante)
    print(f"    Cluster predominante: {cluster_predominante} ({count_predominante} - {pct:.1f}%) - {nome[:40]}...")

# === VisualizaÃ§Ã£o ===
plt.figure(figsize=(15, 10))

if n_clusters <= 20:
    unique_clusters = sorted(set(final_labels))
    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_clusters)))
    
    for i, cluster_id in enumerate(unique_clusters):
        cluster_data = df_result[df_result['cluster'] == cluster_id]
        if cluster_id == -1:
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'], 
                       c='red', marker='x', s=10, alpha=0.6, 
                       label=f'RuÃ­do ({len(cluster_data)})')
        else:
            nome_cluster = cluster_info_dinamico[cluster_id]['nome'][:20]  # Truncar para visualizaÃ§Ã£o
            plt.scatter(cluster_data['pca_1'], cluster_data['pca_2'],
                       c=[colors[i]], s=15, alpha=0.7, 
                       label=f'C{cluster_id}: {nome_cluster} ({len(cluster_data)})')
    
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering por ID_ÃšNICO - {best_algorithm} | Score: {algorithms_scores[best_algorithm]:.3f}')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    plt.grid(True, alpha=0.3)
else:
    plt.scatter(df_result['pca_1'], df_result['pca_2'], 
               c=final_labels, cmap='tab20', s=30, alpha=0.7)
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.title(f'Clustering por ID_ÃšNICO - {best_algorithm} ({n_clusters} clusters)')
    plt.colorbar()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# === RELATÃ“RIO DE QUALIDADE DOS DADOS ===
print(f"\n=== RELATÃ“RIO DE QUALIDADE DOS DADOS ===")
print(f"ğŸ“‹ Total de registros originais: {len(df):,}")
print(f"ğŸ”‘ IDs Ãºnicos criados: {df['ID_UNICO'].nunique():,}")
print(f"ğŸ§¹ IDs Ãºnicos apÃ³s limpeza: {len(df_clean):,}")
print(f"ğŸ“Š IDs Ãºnicos com cluster: {len(df_result):,}")
print(f"ğŸ“ IDs Ãºnicos no arquivo final: {len(resultado_final_completo):,}")

duplicatas_freq = df.groupby(['Freq', 'Chave_Site']).size()
sites_com_duplicatas = (duplicatas_freq > 3).sum()  # Sites com mais de 3 setores
print(f"ğŸ—ï¸ Sites Ãºnicos: {df['Chave_Site'].nunique():,}")
print(f"ğŸ“¡ FrequÃªncias Ãºnicas: {df['Freq'].nunique()}")
print(f"ğŸ“¶ Setores Ãºnicos: {df['Setor'].nunique()}")
print(f"âš ï¸ Sites com mais de 3 setores: {sites_com_duplicatas}")

# === RECOMENDAÃ‡Ã•ES FINAIS ===
print(f"\n=== RECOMENDAÃ‡Ã•ES FINAIS ===")
print(f"âœ… DADOS PROCESSADOS COM SUCESSO!")
print(f"ğŸ”‘ Chave Ãºnica ID_UNICO criada: Freq + Chave_Site + Setor")
print(f"ğŸ“Š {n_clusters} clusters identificados dinamicamente")
print(f"ğŸ¯ Priorize clusters com classificaÃ§Ã£o 'CRÃTICO' e 'PROBLEMÃTICO'")
print(f"ğŸ“ Arquivo salvo: '{filename}' - Pronto para anÃ¡lise!")

print(f"\nğŸ” PRÃ“XIMOS PASSOS SUGERIDOS:")
print(f"1. Analise os clusters CRÃTICOS primeiro (prioridade ALTA)")
print(f"2. Investigue outliers (-1) individualmente")
print(f"3. Compare performance entre sites da mesma frequÃªncia")
print(f"4. Monitore evoluÃ§Ã£o temporal dos clusters")
print(f"5. Use ID_UNICO para rastreamento individual")

print(f"\nğŸ‰ ANÃLISE COMPLETA COM CHAVE ÃšNICA!")
print(f"ğŸ“„ Arquivo final: '{filename}'")
print(f"ğŸ¤– Algoritmo: {best_algorithm}")
print(f"ğŸ“Š Score: {algorithms_scores[best_algorithm]:.3f}")
print(f"ğŸ”¢ Clusters identificados: {n_clusters}")
print(f"ğŸ”‘ IDs Ãºnicos processados: {len(resultado_final_completo):,}")
print(f"ğŸ“ˆ Todos os clusters sÃ£o nomeados DINAMICAMENTE baseados nos dados reais!")
print(f"ğŸ”„ CÃ³digo adaptado para usar chave Ãºnica Freq+Chave_Site+Setor!")
